; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc -mtriple=amdgcn-amd-amdhsa -mcpu=gfx90a -verify-machineinstrs=0 -O0 2> %t.err < %s | FileCheck %s

; FIXME: This error will be fixed by supporting arbitrary divergent
; dynamic allocas by performing a wave umax of the size.


define i32 @move_to_valu_assert_srd_is_physreg_swdev503538(ptr addrspace(1) %ptr) {
; CHECK-LABEL: move_to_valu_assert_srd_is_physreg_swdev503538:
; CHECK:       ; %bb.0: ; %entry
; CHECK-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; CHECK-NEXT:    s_mov_b32 s7, s33
; CHECK-NEXT:    s_mov_b32 s33, s32
; CHECK-NEXT:    s_xor_saveexec_b64 s[4:5], -1
; CHECK-NEXT:    buffer_store_dword v3, off, s[0:3], s33 ; 4-byte Folded Spill
; CHECK-NEXT:    s_mov_b64 exec, s[4:5]
; CHECK-NEXT:    s_add_i32 s32, s32, 0x400
; CHECK-NEXT:    v_mov_b32_e32 v2, v1
; CHECK-NEXT:    ; implicit-def: $sgpr4
; CHECK-NEXT:    ; implicit-def: $sgpr4
; CHECK-NEXT:    ; kill: def $vgpr0 killed $vgpr0 def $vgpr0_vgpr1 killed $exec
; CHECK-NEXT:    ; kill: def $vgpr1 killed $vgpr2 killed $exec
; CHECK-NEXT:    ; implicit-def: $sgpr4_sgpr5
; CHECK-NEXT:    v_mov_b32_e32 v0, s32
; CHECK-NEXT:    v_accvgpr_write_b32 a0, v0 ; Reload Reuse
; CHECK-NEXT:    v_readfirstlane_b32 s32, v0
; CHECK-NEXT:    v_accvgpr_write_b32 a1, v0 ; Reload Reuse
; CHECK-NEXT:    ; implicit-def: $sgpr4
; CHECK-NEXT:    s_mov_b64 s[4:5], exec
; CHECK-NEXT:    ; implicit-def: $vgpr3 : SGPR spill to VGPR lane
; CHECK-NEXT:    v_writelane_b32 v3, s4, 0
; CHECK-NEXT:    v_writelane_b32 v3, s5, 1
; CHECK-NEXT:    s_or_saveexec_b64 s[10:11], -1
; CHECK-NEXT:    v_accvgpr_write_b32 a2, v3 ; Reload Reuse
; CHECK-NEXT:    s_mov_b64 exec, s[10:11]
; CHECK-NEXT:  .LBB0_1: ; =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    s_or_saveexec_b64 s[10:11], -1
; CHECK-NEXT:    v_accvgpr_read_b32 v3, a2 ; Reload Reuse
; CHECK-NEXT:    s_mov_b64 exec, s[10:11]
; CHECK-NEXT:    v_accvgpr_read_b32 v0, a0 ; Reload Reuse
; CHECK-NEXT:    v_readfirstlane_b32 s4, v0
; CHECK-NEXT:    v_writelane_b32 v3, s4, 2
; CHECK-NEXT:    v_cmp_eq_u32_e64 s[4:5], s4, v0
; CHECK-NEXT:    s_and_saveexec_b64 s[4:5], s[4:5]
; CHECK-NEXT:    v_writelane_b32 v3, s4, 3
; CHECK-NEXT:    v_writelane_b32 v3, s5, 4
; CHECK-NEXT:    s_or_saveexec_b64 s[10:11], -1
; CHECK-NEXT:    v_accvgpr_write_b32 a2, v3 ; Reload Reuse
; CHECK-NEXT:    s_mov_b64 exec, s[10:11]
; CHECK-NEXT:  ; %bb.2: ; in Loop: Header=BB0_1 Depth=1
; CHECK-NEXT:    s_or_saveexec_b64 s[10:11], -1
; CHECK-NEXT:    v_accvgpr_read_b32 v3, a2 ; Reload Reuse
; CHECK-NEXT:    s_mov_b64 exec, s[10:11]
; CHECK-NEXT:    v_readlane_b32 s4, v3, 3
; CHECK-NEXT:    v_readlane_b32 s5, v3, 4
; CHECK-NEXT:    v_readlane_b32 s6, v3, 2
; CHECK-NEXT:    s_nop 4
; CHECK-NEXT:    buffer_load_dword v0, off, s[0:3], s6
; CHECK-NEXT:    s_waitcnt vmcnt(0)
; CHECK-NEXT:    v_accvgpr_write_b32 a3, v0 ; Reload Reuse
; CHECK-NEXT:    s_xor_b64 exec, exec, s[4:5]
; CHECK-NEXT:    s_cbranch_execnz .LBB0_1
; CHECK-NEXT:  ; %bb.3:
; CHECK-NEXT:    s_or_saveexec_b64 s[10:11], -1
; CHECK-NEXT:    v_accvgpr_read_b32 v3, a2 ; Reload Reuse
; CHECK-NEXT:    s_mov_b64 exec, s[10:11]
; CHECK-NEXT:    v_readlane_b32 s4, v3, 0
; CHECK-NEXT:    v_readlane_b32 s5, v3, 1
; CHECK-NEXT:    s_mov_b64 exec, s[4:5]
; CHECK-NEXT:    s_mov_b32 s4, 0
; CHECK-NEXT:    v_writelane_b32 v3, s4, 5
; CHECK-NEXT:    s_or_saveexec_b64 s[10:11], -1
; CHECK-NEXT:    v_accvgpr_write_b32 a2, v3 ; Reload Reuse
; CHECK-NEXT:    s_mov_b64 exec, s[10:11]
; CHECK-NEXT:  .LBB0_4: ; %loadstoreloop
; CHECK-NEXT:    ; =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    s_or_saveexec_b64 s[10:11], -1
; CHECK-NEXT:    v_accvgpr_read_b32 v3, a2 ; Reload Reuse
; CHECK-NEXT:    s_mov_b64 exec, s[10:11]
; CHECK-NEXT:    v_readlane_b32 s4, v3, 5
; CHECK-NEXT:    v_accvgpr_read_b32 v0, a1 ; Reload Reuse
; CHECK-NEXT:    v_add_u32_e64 v1, v0, s4
; CHECK-NEXT:    v_mov_b32_e32 v0, 0
; CHECK-NEXT:    buffer_store_byte v0, v1, s[0:3], 0 offen
; CHECK-NEXT:    s_mov_b32 s5, 1
; CHECK-NEXT:    s_add_i32 s4, s4, s5
; CHECK-NEXT:    s_mov_b32 s5, 0x800
; CHECK-NEXT:    s_cmp_lt_u32 s4, s5
; CHECK-NEXT:    v_writelane_b32 v3, s4, 5
; CHECK-NEXT:    s_mov_b64 s[10:11], exec
; CHECK-NEXT:    s_mov_b64 exec, -1
; CHECK-NEXT:    v_accvgpr_write_b32 a2, v3 ; Reload Reuse
; CHECK-NEXT:    s_mov_b64 exec, s[10:11]
; CHECK-NEXT:    s_cbranch_scc1 .LBB0_4
; CHECK-NEXT:  ; %bb.5: ; %Flow
; CHECK-NEXT:  ; %bb.6: ; %split
; CHECK-NEXT:    v_accvgpr_read_b32 v0, a3 ; Reload Reuse
; CHECK-NEXT:    s_mov_b32 s32, s33
; CHECK-NEXT:    s_xor_saveexec_b64 s[4:5], -1
; CHECK-NEXT:    buffer_load_dword v3, off, s[0:3], s33 ; 4-byte Folded Reload
; CHECK-NEXT:    s_mov_b64 exec, s[4:5]
; CHECK-NEXT:    s_mov_b32 s33, s7
; CHECK-NEXT:    s_waitcnt vmcnt(0)
; CHECK-NEXT:    s_setpc_b64 s[30:31]
entry:
  %idx = load i32, ptr addrspace(1) %ptr, align 4
  %zero = extractelement <4 x i32> zeroinitializer, i32 %idx
  %alloca = alloca [2048 x i8], i32 %zero, align 8, addrspace(5)
  %ld = load i32, ptr addrspace(5) %alloca, align 8
  call void @llvm.memset.p5.i32(ptr addrspace(5) %alloca, i8 0, i32 2048, i1 false)
  ret i32 %ld
}

declare void @llvm.memset.p5.i32(ptr addrspace(5) nocapture writeonly, i8, i32, i1 immarg) #0

attributes #0 = { nocallback nofree nounwind willreturn memory(argmem: write) }
